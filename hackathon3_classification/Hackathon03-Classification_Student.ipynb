<font size=6  color=#003366> <b>[LEPL1109] - STATISTICS AND DATA SCIENCES</b> <br><br> 
<b>Hackathon 03 - Classification: AirQuality</b> </font> <br><br><br>

<font size=5  color=#003366>
Prof. D. Hainaut<br>
Prof. L. Jacques<br>

<br><br>
Anne-Sophie Collin   (anne-sophie.collin@uclouvain.be)<br>
Guillaume Van Dessel (guillaume.vandessel@uclouvain.be)<br>
Jérome Eertmans (jerome.eertmans@uclouvain.be)<br>
Maxime Zanella (maxime.zanella@uclouvain.be)<br>
Florine Thiry (florine.thiry@uclouvain.be)<br>
Baptiste Standaert (baptiste.standaert@uclouvain.be)<br>
Antoine Legat (antoine.legat@uclouvain.be)<br>
<div style="text-align: right"> Version 2022</div>

<br><br>
</font>
<font size=5 color=#009999> <b>GUIDELINES & DELIVERABLES</b> </font> <br>
-  This assignment is due on the <b>9 December 2022 at 22h00</b>.
-  Copying code or answers from other groups (or from the internet) is strictly forbidden. <b>Each source of inspiration (stack overflow, git, other groups,...) must be clearly indicated!</b>
-  This notebook (with the "ipynb" extension) file, the report (PDF format) and all other files that are necessary to run your code must be delivered on <b>Moodle</b>.
- Only the PDF report will be graded on content and quality of the text / figures. <br><br>

<div class="alert alert-danger">
<b>[DELIVERABLE] Summary</b>  <br>
After the reading of this document (and playing with the code!), we expect you to provide us with:
<ol>
   <li> a PDF file (written in LaTeX, see example on Moodle) that answers all the questions below. The report should contain high quality figures with named axes (we recommend saving plots with the <samp>.pdf</samp> extension);
   <li> this Jupyter Notebook (it will not be read, just checked for plagiarism);
   <li> and all other files (not the datasets!) we would need to run your code.
</ol>
</div>

<font size=5 color=#009999> <b>CONTEXT & OBJECTIVE </b> </font> <br>
    

### Context

The air in cities is polluted by human activities. Vehicular emissions, heating systems, industries... You probably remember those times when the air is so polluted that it is recommended to stay at home, and not do too much sport outside. It is often related to specific weather conditions (e.g. not enough wind to blow air pollution away).

Besides, one could wonder: is it healthy to go running in a city? Do the benefits of sport balance the fact that you are breathing polluted air? Studies show that the answer is yes, as long as you avoid busy roads with dense traffic.

Air quality is a crucial issue in our modern society, and its effect is poorly anchored in common knowledge. As engineers of tomorrow, you must be aware of the impact of thermic engines and industries on human health.

![](https://api.brusselstimes.com/wp-content/uploads/2022/01/763-45.jpeg)

In this hackathon, we will learn to quantify air quality. Wikipedia says that smog (or smoke fog) is composed of nitrogen oxides, sulfur oxide, ozone, smoke and other particulates. How to quantify from those different features?

People use the Air Quality Index (AQI). This is a natural number running from 0 to 500+, the lower the better. AQI accounts for the concentration of important pollutants and particles. We usually distinguish 6 classes of air quality, from good to severe (see table below). In this hackathon, we will classify the AQI between only 2 classes: good or bad.


![](https://i.imgur.com/XmnE0rT.png)
 


### Objectives

The project aims to train a binary classifier to estimate the air quality index (AQI) based on the air concentration of certain pollutants. Note that the AQI is initially defined as a natural number between 0 and 500+. However, to better address the problem, we propose to classify the air quality as poor (labeled 0) or good (labeled 1). The next part of the document will guide you in this process.  

### Notebook structure
This notebook is organized into four parts. Each of them assesses one fundamental step to solve our problem and provides one visualization tool to gain some understanding:
* PART 1 - PREPROCESSING
   - 1.1 - Import the dataset
   - 1.2 - Split the dataset
    <br><br>
* PART 2 - EXPLORATORY DATA ANALYSIS 
   - 2.1 - Binary targets
   - 2.2 - Cyclical features
   - 2.3 - Correlation matrix
   - 2.4 - Features selection
   - 2.5 - Data scaling and normalization
    <br><br>
* PART 3 - MODEL SELECTION
   - 3.1 - Precision, recall and F1-score
   - 3.2 - Model evaluation
   - 3.3 - Model selection and parameters tuning
   - 3.4 - Precision-Recall curve and thresholding
   <br><br>
* PART 4 - MODEL TESTING
   - 4.1 - Error computation on the test set
   
We filled this notebook with preliminary (trivial) code. This practice makes possible to run each cell, even the last ones, without throwing warnings. <br><b>Take advantage of this aspect to divide the work between all team members!</b> <br><br> 
## warnings off
import warnings
warnings.filterwarnings("ignore")
<br><font size=7 color=#009999> <b>PART 1 - DATA LOADING</b> </font> <br><br>
<font size=4 color=#009999> <br> 1.0/1 DISCOVER THE DATASET </font> <br>

**Import** `gas_measurements.csv` using `read_csv` [<sup>1</sup>](#fn1) from pandas and **obtain** a brief description of the data (size, variables type, missing values, etc.).  



<span id="fn1"> [1] N.B : the separator in the csv file is ',' and not the default comma (see the *`sep`* argument).</span>
<div class="alert alert-warning">
    <b>[Question 1.0]</b> Describe, briefly, your dataset (size, variables type, missing values, etc.).<br>
</div> 
"""
CELL N°1 : Import the dataset using pd.read_csv function 

@pre: filename 'gas_measurements.csv', located in the same folder as this jupyter
@post: variable `df` containing the dataframe
"""

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn.datasets import load_iris

#############################
# Start : 
#############################

# @rewrite

#features = ['BigUrbanCity', 'PM10', 'PM2_5', 'SO2', 'NOx', 'NH3', 'CO', 'O3', 'Datetime', 'AQI']
fullData = pd.read_csv("gas_measurements.csv")
#remove first colomn
fullData = fullData.iloc[: , 1:]
fullData.head()
#fullData.info()

##########################
# End : 
#########################
<div class="alert alert-warning">
    <b>[Question 1.1]</b> Based on your observations, justify and proceed to the <i>a priori</i> deletion of <b>two</b> troublesome features. 
</div> 

<i>HINT</i>: one usually decides of a <i>threshold</i> $\tau\in [0,1[$ of the allowed proportion of missing data within a given feature...

<b>NOTE</b>: there will remain other features with missing data.. it is not a big deal as soon as the proportion of missing data stays <i>reasonable</i>. In what concerns these features with missing data that are not discarded here, we will proceed with a <i>fill-in</i> operation later on.
#############################
# Start : 
#############################

# @rewrite
tau = 0


##########################
# End : 
#########################
<font size=4 color=#009999> <br> 1.2 - SPLIT THE DATASET </font> <br> 

Data science projects begin by the division the **whole** dataset into a **training** and a **test** set. The subsequent analysis and decisions (i.e. features selection, pre-processing, model selection, etc.) are, then, conducted only on the _training set_ to stay statistically significant during the **testing phase**. The latter will, thus, only be conducted on the _test set_.  

We invite you then to **split** [<sup>2</sup>](#fn2) the dataset into a _training_ and a _test_. The proportion of each subset is at **your own discretion**.


<span id="fn2"> [2] N.B. Set the seed of your random split with `random_state = 42` to obtain reproducible results.</span>



<div class="alert alert-warning">
    <b>[Question 1.2]</b> What are the drawbacks (if any) of choosing a small test set (in proportion)? On the contrary, what are the consequences (if any) of a relatively large testing set (in proportion)? <br>
</div> 
"""
CELL N°2 : SPLIT THE DATASET    

@pre:  'df' a pandas frame with the entire dataset
@post: 2 pandas frames with the train and test sets
"""

from sklearn.model_selection import train_test_split

#############################
# Start : 
#############################

# @rewrite

data_train, data_test = train_test_split(fullData, test_size=0.3, random_state=42, shuffle=True)

##########################
# 
#########################
<br>

<br><font size=7 color=#009999> <b>PART 2 - EXPLORATORY DATA ANALYSIS </b> </font> <br><br>


We **analyze** the distribution of the features _AQI_ in the binary scenario. Namely, air quality is considered either <ol> <li>**bad** (AQI : Poor, Very Poor, Severe) </li> <li>**good** (AQI : Good, Satisfactory, Moderate)</li> </ol>

We conduct the analysis on the <i>training set</i>, avoiding therefore any modelling decision based on _unseen_ data (<i>test set</i>). In most cases, we assume that the distribution of this latter set stays similar to the <i>training set</i>.
<font size=4 color=#009999> <br> 2.1. Binary targets  </font> <br>

<div class="alert alert-warning">
    <b>[Question 2.1]</b> Are the binary classes balanced? What are the proportions of data in each class? Briefly, justify your answer and add a visualization.
</div> 

"""
CELL N°3 : Binary targets: proportion and binarization
   
@pre:  Training dataframe  
@post: Proportion of each binary class in this train set, a pie chart representing it, 
a modified training dataframe with a new column 'AQI_binary' continaining the binary targets 
"""

#############################
# Start :
#############################

# @rewrite
import matplotlib.pyplot as plt
import numpy as np

data_train['AQI'] = data_train['AQI'].replace(['Severe'], 0)
data_train['AQI'] = data_train['AQI'].replace(['Very Poor'], 0)
data_train['AQI'] = data_train['AQI'].replace(['Poor'], 0)
data_train['AQI'] = data_train['AQI'].replace(['Moderate'], 1)
data_train['AQI'] = data_train['AQI'].replace(['Satisfactory'], 1)
data_train['AQI'] = data_train['AQI'].replace(['Good'], 1)

y_train = data_train['AQI']
y_train_binary = np.bincount(y_train) 

plt.title("Pie Chart - AQI")
plt.pie(y_train_binary, labels = ["BAD","GOOD"], autopct='%1.1f%%')
plt.show() 

##########################
# End : 
#########################
<font size=4 color=#009999> 2.2. Cyclical features </font> <br>

<b>Establish</b> a <i>cyclical</i> feature transformation and store the new features in the corresponding variables. 

<span id="fn3"> Many features are cyclical in nature. One example is time: months, days, weekdays, hours, minutes, seconds etc. are all cyclical.</span> 
<div class="alert alert-warning">
    <b>[Question 2.2]</b> Explain your transformation and the need to use cyclical features in some cases. What is the point to not simply encode features in a categorical way (e.g. morning=0, afternoon=1, evening=2, night=3)? 
</div>  

<i>HINT</i>: what happens between 23:00:00 and 00:00:00 ?
"""
HINT CELL : This cell is intended to help you understand CELL N°4 (bis) and to answer the corresponding questions. 
Observe how the final plot looks like a clock ;)
"""

hours = [i for i in range(24)]*2
plt.scatter(np.arange(len(hours)),hours,label='data points')
plt.plot(np.arange(len(hours)),hours,color='red')
plt.grid()
plt.title('Visualization')
plt.legend()
plt.ylabel('hour of the corresponding day')
plt.xlabel('cumulative hour')
plt.show()


sin_hours = np.sin(2 * np.pi * np.array(hours)/24.0)
cos_hours = np.cos(2 * np.pi * np.array(hours)/24.0)

plt.subplot(2,2,1)
plt.plot(hours[0:24], sin_hours[0:24])
plt.title('sinus component')
plt.subplot(2,2,2)
plt.plot(hours[0:24], cos_hours[0:24])
plt.title('cosinus component')

ax, fig = plt.subplots()
fig.set_aspect('equal')
plt.xlabel('sin_hour')
plt.ylabel('cos_hour')
plt.scatter(sin_hours[0:24], cos_hours[0:24])
plt.show()

"""
CELL N°4 : Time feature conversion
   
@pre:  Training and testing dataframe
@post: Training and testing dataframe with a new colum 'hour' filled with the corresponding hour of the day (integer)

DO NOT FORGET TO APPLY TRANSFORMATIONS TO BOTH data_train AND data_test
"""
from datetime import datetime

def datetime_to_hour(date_time):
    hours = []
    for date in date_time:
        datetime_obj = datetime.strptime(date, '%Y-%m-%d %H:%M:%S')
        hours.append(datetime_obj.hour)
    return hours

#############################
# Start : 
#############################

# @toDo

##########################
# End : 
########################
"""
CELL N°4 (BIS) : Cyclical features transformation
   
@pre:  Training and testing dataframe
@post: Training and testing dataframe with two new columns 'sin_hour' and 'cos_hour' with hours of the day encoded in a cyclical way
       
Afterwards, you can drop the column 'Datetime', useless from now on :) 

DO NOT FORGET TO APPLY TRANSFORMATIONS TO BOTH data_train AND data_test
"""

def categorical_to_cyclical(features, max_value=24):
    sin_features = np.sin(2 * np.pi * features/max_value)
    cos_features = np.cos(2 * np.pi * features/max_value)
    return (sin_features, cos_features)

#############################
# Start : 
#############################

# @toDo

##########################
# End : 
########################
<font size=4 color=#009999> <br> 2.3. Correlation matrix </font> <br>

__Compute__ and __plot__ the correlation matrix. For the plot, you can use the function `imshow` or `matshow` from `matplotlib`.
<div class="alert alert-warning">
    <b>[Question 2.3]</b> With the help of your plot and numerical results of cell 4, write your observations down.  <br
                                                                                                                
</div> 
"""
CELL N°5 : CORRELATION MATRIX
   
@pre:  Training dataframe  
@post: The correlation matrix between the features (target incl.) and its plot    
"""
import matplotlib.pyplot as plt

# Compute and plot the correlation matrix
# Display the correlation in cells


#############################
# Start : 
#############################

fig = plt.figure(figsize=(10, 10))
plt.matshow(data_train.corr(), fignum=fig.number, cmap='coolwarm')

plt.xticks(range(data_train.shape[1]), data_train.columns, fontsize=9, rotation=20)
plt.yticks(range(data_train.shape[1]), data_train.columns, fontsize=9)
cb = plt.colorbar()
plt.title('Correlation Matrix', fontsize=14);

##########################
# End : 
#########################
<font size=4 color=#009999> 2.4. Features Selection </font> <br>

<b>Establish</b> a feature selection strategy and store the feature names in a list. 

N.B. Selection of the $n$ first correlated features and/or setting of a correlation threshold or any other rule. <br>Furthermore, investigate on <i>redundant</i> features which can be removed as well.</span> 
<div class="alert alert-warning">
    <b>[Question 2.4]</b> Explain your selection strategy. 
    Is it appropriate to use the correlation matrix to select (or not) cyclical features?
</div> 
"""
CELL N°6 : Features selection 
   
@pre:  data_train's correlation matrix: correl_mat (see CELL  N°5)
@post: a list 'feature_names' with the names of the selected features, display of each explanatory feature vs. binary target
"""

#############################
# Start : Only for the TA's
#############################

# @toDo (implement your feature selection rule)


# Ici, nous avons retiré les features de temporalité car elles sont peu corrélées; normal elles sont sous la forme d'un sinus/cosinus.
# Cependant, une selection plus "approfondie" aurait pu capturer des relations qui ne sont pas uniquement linéaires.

def feature_selection(df,n):
    cm = abs(df.corrwith(df["AQI"]))
    cm = cm.drop("AQI")
    out = cm.sort_values()
    return out[-n:]

feature_names = feature_selection(data_train,5)

print(' ')
print('auto selected features: ' + '\n' +str(feature_names[::-1]))

##########################
# End : Only for the TA's
########################
<font size=4 color=#009999> <br> 2.5 Data scaling and normalization</font> <br> 

__Split__ your _training_ and _test_ sets into their respective features set  (X)  and a binary target variable (y). __Standardize__ or __Normalize__ the features sets. 


__Remark 1.__ The scaler object, used to scale the <i>training set</i>, should also be the one used on the <i>test set</i>! Again, do no reinvent the wheel!  

<div class="alert alert-warning">
    <b>[Question 2.5]</b>  Why do we scale data? Justify properly, whether it is necessary or not for your feature set (X) and which scaler did you use.
</div> 
"""
CELL N°7 : Data fill-in & scaling
   
@pre: train and test dataframes and the list 'feature_names' of columns to keep    
@post:  X_train: numpy array, with (scaled) selected features, containing training data
        X_test: numpy array, with (scaled) selected features, containing testing data. 
                The scaling should be done using the statistic of the train set.
"""

from sklearn.preprocessing import StandardScaler, MinMaxScaler


## filling the rest of missing-values (not much) with median value
for col in feature_names:
    med_col = data_train[col].median()
    data_train[col] = data_train[col].fillna(med_col)
    data_test[col] = data_test[col].fillna(med_col)

## Keeping only the selected features
X_train = data_train[feature_names]
X_test = data_test[feature_names]

#############################
# Start : 
#############################


# @toDo


##########################
# End : 
#########################
<font size=4 color=#009999>  </font> <br>

<br><font size=7 color=#009999> <b>PART 3 - MODEL SELECTION </b> </font> <br><br>


Let us first build some tools that will help us to choose among our investigated models together with their (hyper-)parameters which one performs the best. 
<font size=4 color=#009999> <br> 3.1. PRECISION, RECALL AND F1 SCORE </font> <br>

**Implement** the _precision, recall_ and _F-measure_ metrics based on the confusion matrix. Please follow the specifications in the provided template.  <br>

**Reminder**

$F_1$ is a performance score allowing to obtain some trade-off between the precision and recall criterions. It can be computed as follows:
$$F_1 = 2~\frac{\mathrm{precision} \cdot \mathrm{recall}}{\mathrm{precision} + \mathrm{recall}}.$$

Please consult, [Wikipedia](https://en.wikipedia.org/wiki/F-score) for further information about the metric.

**NOTE**: if ever the model you built would be terribly <i>bad</i>, with both precision's and recall's value equal to $0$, we suggest to return $0$ as $F_1$ score. 
"""
CELL N°9 : Implementation of precision, recall & F1 scores

@pre:  /  
@post: Follow the specifications to implement precision, recall and probas_to_F1 functions. 
"""
from sklearn.metrics import confusion_matrix

""" -----------------------------------------------------------------------------------------
Converts a vector of real probability values to a binary 0 or 1 
@pre: 
    - proba_vec: vector with real values representing each a probability that the air quality is good.
    - threshold : a threshold probability (between 0 and 1) to determine if a given air is good (1) or not (0)
@post:
    - predicted_labels: binary prediction vector, with elements being 0 or 1.
----------------------------------------------------------------------------------------- """


def pred_probas_to_pred_labels(proba_vec, threshold=0.5):
    return np.where(proba_vec <= threshold, 0, 1)


""" -----------------------------------------------------------------------------------------
Based on the confusion matrix, computes the 'precision'
@pre: 
    - cm : confusion_matrix of a binary classification
@post:
    - score: precision (or positive predictive value), associated with cm
----------------------------------------------------------------------------------------- """


def precision(cm):

    ###########################
    # Start :
    ###########################
    # @rewrite
    ppv = np.random.uniform(0,1)
    ###########################
    # End : 
    ###########################

    return ppv


""" -----------------------------------------------------------------------------------------
Based on the confusion matrix, computes the 'recall'
@pre: 
    - cm : confusion_matrix of a binary classification  
@post:
    - r: recall (or true positive rate), associated with cm
----------------------------------------------------------------------------------------- """


def recall(cm):

    ###########################
    # Start : 
    ###########################
    # @rewrite
    tpr = np.random.uniform(0,1)
    ###########################
    # End : 
    ###########################
    return tpr


""" -----------------------------------------------------------------------------------------
Evaluates the F1 score which is a harmonic mean of the precision and recall
@pre: 
    - y_true: vectors of 0 and 1 representing the real class values
    - y_pred: vectors of real values representing predicted probability of the air being good ('1')
    - output:  'F1' means that the output should only be the F1 score. 
               'PRF1' means that the output is a tuple with (precision, recall, F1)
               'F1' is the default value
    - threshold: a threshold probability (between 0 and 1) to determine if the air is good ('1') or not ('0')
@post:
    - F1_score: harmonic mean of the precision and recall
    - If asked in argument, precision and recall can be added in the output: (precision, recall, F1)
----------------------------------------------------------------------------------------- """


def probas_to_F1(y_true, y_pred, output="F1", threshold=0.5):

    y_pred = pred_probas_to_pred_labels(y_pred, threshold)

    ###########################
    # Start : 
    ###########################
    # @rewrite 
    cm = np.random.randint(0,20,size=(2,2))
    tpr = 0
    ppv = 0
    F1_score = 0

    if output == "PRF1":
        return (ppv, tpr, F1_score)
    
    ###########################
    # End : 
    ###########################

    return F1_score
<font size=4 color=#009999> <br> 3.2. MODEL EVALUATION  </font> <br>

**Implement** `evalParam`, which evaluates, using a __k-fold__ cross-validation, a list of `scikit-learn` models. Use your method `probas_to_F1` as score function. The function `evalParam` must be  **scalable**. Put differently, it must handle $m$ methods, and a variable list of their possible parameters configuration. 


In addition of the list of _models_ (methods) and their list of _hyperparameters_ (param), the function takes as arguments the _features set_ (X), _target variable_ (y) and _the number of folds_ (cv). 

It returns an array _score_ such that <br>

$$score[i][j] = average F1 over the folds, using method _i_ with parameters configuration j.$$

To help you, here is a pseudo code of K-fold for one method and one configuration of hyperparameters. 

<img src="K-fold_pseudo-code.png" width = "650">
 
__Remark 1.__ You have to implement a K-fold cross-validation. You are only allowed to use `KFold.splits(dataset)` from `sklearn.model_selection` to generate the indices of your different folds. 

<div class="alert alert-warning">
    <b>[Question 3.1]</b>
    Explain the idea of K-fold cross-validation and why it is useful. How the choice of K (in the cross-validation) impacts the bias and the variance of the scores obtained on the different folds? Choose and justify the number of folds you consider in this project. 
</div> 
"""
CELL N°10 : Evaluates the methods using different parameters via a K-folds with cv folds

@pre: 
    - methods: list of classifiers to analyze
    - param: list of size len(methods) containing lists of parameters (in dictionary form) to evaluate.
             In other words, param[i][j] is a dictionary of parmeters.
             For example if index i is for KNN, we can have a parameter configuration (with index j) described as
                 param[i][j] = {"n_neigbors":5, "weights": 'uniform'}; 
                 while param[i] is a list of such parameters dictionnaries for model i (here KNN)
    - X: training dataset
    - y: target vector for the corresponding entries of X
    - cv: the number of folds to use in your cross-validation
@post:
    - score: list with same shape as param. score[i][j] = mean score over the folds, 
                                                         using method i with parameters param[i][j]
------------------------------------------------------------------------------------------------ """
from sklearn.model_selection import KFold


def evalParam(methods, param, X, y, cv):

    #############################
    # Start : 
    #############################
    
    score = []
    for i in range(len(methods)):
        score.append(np.zeros(len(param[i])))
        
    # @rewrite

    for meth in range(len(methods)):
        for p in range(len(param[meth])):
            score[meth][p] = 0
    ##########################
    # End : 
    ########################
    return score
<font size=4 color=#009999> <br> 3.3. MODEL SELECTION AND PARAMETERS TUNING </font> <br>

__Run__ your function `evalParam` to evaluate the three following models : [_linear regression_](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html#sklearn.linear_model.LinearRegression), [_logistic regression_](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [_K nearest neighbors_](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html). <br>These models are already implemented in sklearn. <br>

**Study** the effect of the following hyperparameters:
- `n_neighbors` in KNN (try selected values between 1 and 100),
- `weights` in KNN (try both values 'uniform' and 'distance')
- `p` in KNN (try euclidean (p=2), manhattan (p=1) and minkowski-100 (more or less equivalent to max-norm))
- `C` in logistic regression (try selected values between $10^{-3}$ and $10^3$)</li>


<div class="alert alert-warning">
    <b>[Question you should ask yourself]</b> (Not graded) Prior to the run, discuss the fitness of each model to answer to our problem. 
</div> 

<div class="alert alert-warning">
    <b>[Question 3.2]</b> Explain your methodology of model evaluation. More precisely, explain which hyperparameters you tune and the values you test for each of them. Next, provide the best hyperparameters configuration for each of the three models as well as their CV F1 score.
</div>

"""
CELL N°11 : Model selection - tuning the three methods
   
@pre: evalParam function correctly implemented    
@post:  three models (knn, linear and logistic regression) initialized with tuned hyperparameters.
        print the best hyperparameters found, as well as their CV F1 scores associated with these hyperparameters.
------------------------------------------------------------------------------------------------ """

from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier

#############################
# Start : 
#############################

# @rewrite
linreg = LinearRegression()  
methods = [linreg]
paramLR = [{"normalize": False}]  # no tuning to do for linreg but we put a default parameters here too
param = [paramLR]
cv = 1 


##########################
# End : 
########################

print(' ')


#############################
# Start : 
#############################

cv_scores = evalParam(methods, param, X_train, y_binary_train, cv)

# @toAdapt for linear regression
LRbest = 0  # to do
print(paramLR[LRbest], "with score {0:.4f}".format(cv_scores[0][LRbest]))
linreg.set_params(**paramLR[LRbest])

# @toDo for KNN and Logistic regression


##########################
# End :
########################
<div class="alert alert-warning">
    <b>[Question 3.3]</b> Based on your answers to previous questions, select a final model that you will keep as classifier. Justify.
</div> 
<font size=4 color=#009999> <br> 3.4. Precision, Recall and thresholding </font> <br>

In general, the classifying models compute first the probability for a point to belong to a certain class. Next, they applies a threshold to assign the final label (bad or good). By default, `scikit-learn` applies a threshold of 0.5 for KNN and logistic regression. You can use the function `predict_proba` to obtain the original probabilities. <br>
For analyzing the impact of the threshold on the precision and recall of a model, we generally plot its precision-recall curve. Specific functions in sklearn help doing that plot (see [sklearn documentation](https://scikit-learn.org/stable/auto_examples/model_selection/plot_precision_recall.html)). <br>
Here we only ask you theoretical questions, so we don't expect any plot from you, but feel free to create some for your own reflection and to help you answering correctly.

<div class="alert alert-warning">
    <b>[Question 3.4]</b> What happens to the precision and recall (of any method) when the threshold tends to 0? And when it tends to 1? Explain and, if possible, establish a link with Question 2.1.
</div>

<div class="alert alert-warning">
    <b>[Question 3.5]</b> 
    Explain which precision/recall trade-off you prefer to have for the specific task asked in this hackathon: determining whether air quality is good based on some concentrations. How should you adjust the threshold of your model to bring it closer to the desired trade-off? Should it be above or below the default threshold value of 0.5? <br>
    <b> Note: </b> In the next section, we will keep considering the default threshold value of 0.5.
</div>

"""
TEST CELL N°2 : PRECISION-RECALL CURVES
   
@pre: the three models (knn, linear and logistic regression) initialized with their tuned hyperparameters.  
@post: a figure with the precision-recall curves for the three given models and also for a simple baseline classifier, 
        applied on a validation set containing 10% of the training set.
------------------------------------------------------------------------------------------------ """

from sklearn.metrics import precision_recall_curve
from sklearn.metrics import auc
from sklearn.model_selection import train_test_split

# Validation set to use for the PR curves
X_train2, X_val = train_test_split(X_train, test_size=0.1, random_state=42)
y_train2, y_val = train_test_split(y_binary_train, test_size=0.1, random_state=42)

#############################
# Start : just a few lines asked
#############################

# Training the tuned models
linreg.fit(X_train2, y_train2)
y_scoreLR = linreg.predict(X_val)

# @rewrite (linreg -> logreg, linreg -> knn)
linreg.fit(X_train2, y_train2)
y_scoreLogR = linreg.predict(X_val)

linreg.fit(X_train2, y_train2)
y_scoreKNN = linreg.predict(X_val)

# Computing the precision, recall and threshold for each model
precisionLR, recallLR, thresholdsLR = precision_recall_curve(y_val, y_scoreLR)
pr_aucLR = auc(recallLR, precisionLR)

precisionLogR, recallLogR, thresholdsLogR = precision_recall_curve(y_val, y_scoreLogR)
pr_aucLogR = auc(recallLogR, precisionLogR)

precisionKNN, recallKNN, thresholdsKNN = precision_recall_curve(y_val, y_scoreKNN)
pr_aucKNN = auc(recallKNN, precisionKNN)

# Plotting the precision-recall curves
plt.figure()
plt.plot(
    recallLR,
    precisionLR,
    color="darkorange",
    lw=2,
    label="LR (area = %0.2f)" % pr_aucLR,
)
plt.plot(
    recallLogR,
    precisionLogR,
    color="darkgreen",
    lw=2,
    label="LogR (area = %0.2f)" % pr_aucLogR,
)
plt.plot(
    recallKNN, precisionKNN, color="red", lw=2, label="KNN (area = %0.2f)" % pr_aucKNN
)

plt.plot(
    [0, 1],
    [0.66, 0.66],
    color="navy",
    lw=2,
    linestyle="--",
    label="constant pred (area = 0.66)",
)
plt.xlim([0.0, 1.0])
plt.ylim([0.4, 1.05])
plt.xlabel("Recall")
plt.ylabel("Precision")
plt.title("Precision - Recall curve")
plt.legend(loc="lower right")
plt.show()

#########################
# End : Only for the TA'
########################
The code below only serves as a default setting. <br>You have to rewrite the lines below using your selected model ($\texttt{linreg}, \texttt{recallKNN}, \dots$ should be replaced by the right elements of your choice).
"""
TEST CELL N°3 : THRESHOLDING
   
@pre:   vectors of thresholds, precision and recall scores used for the PR curve in the previous cell
@post:  clf contains your selected classifier.
        thresh is your selected threshold  
------------------------------------------------------------------------------------------------ """


clf = linreg  # @rewrite with the selected model... :) 

#############################
# Start : Only for the TA's
# No code asked to the students
#############################

# we choose the threshold that gives precision >= 0.9, for the largest recall.
idx = np.argmax(recallKNN[precisionKNN >= 0.9])
threshold = thresholdsKNN[precisionKNN[0:-1] >= 0.9][idx]
idt = np.where(thresholdsKNN == threshold)[0][0]
recall_t = recallKNN[idt]
precision_t = precisionKNN[idt]
F1_t = 2 * (recall_t * precision_t) / (recall_t + precision_t)
print(
    "If we use threshold={0:.2f}, we should obtain recall={1:.2f}, precision={2:.2f} and F1={3:.2f}".format(
        threshold, recall_t, precision_t, F1_t
    )
)

#########################
# End : Only for the TA'
########################
<br>

<br><font size=7 color=#009999> <b>PART 4 - MODEL TESTING </b> </font> <br><br>

<div class="alert alert-warning">
    <b>[Question 4.1]</b> Use the test set to estimate the precision, recall and F1 score of your final model and validate its performance on unseen data. <br> Observe if the scores are similar to the ones estimated with your cross-validation.
        Are you satisfied by the performance of your classifier, in view of the task for which it will be used?

</div> 

"""
CELL N°10 : MODEL TESTING
   
@pre:   clf is your selected classifier
        X_test is the numpy array containing the test set (with your selected features)
        y_test is the numpy array contaning your binary target vector
@post:  print the F1, precision and recall on the test set.
------------------------------------------------------------------------------------------------ """

#############################
# Start : 
#############################

# @rewrite
F1 = 0
prec = 0 
rec = 0

# prints
print("--------- For our tuned KNN ---------")
print(f"F1 score: {F1:2.2%}")
print(f"Precision function: {prec:2.2%}")
print(f"Recall function: {rec:2.2%}")

##########################
# End : 
########################
